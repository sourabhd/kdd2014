{
 "metadata": {
  "name": "KDD2014_2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import cPickle as pickle\n",
      "import scipy.sparse as sps\n",
      "from sklearn import linear_model\n",
      "import sklearn.preprocessing as pre\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import linear_model, cross_validation,svm,naive_bayes,ensemble\n",
      "import re \n",
      "\n",
      "from time import time\n",
      "from operator import itemgetter\n",
      "from scipy.stats import randint as sp_randint\n",
      "\n",
      "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(s):\n",
      "        try:\n",
      "            return \" \".join(re.findall(r'\\w+', s,flags = re.UNICODE | re.LOCALE)).lower()\n",
      "        except:\n",
      "            return \" \".join(re.findall(r'\\w+', \"no_text\",flags = re.UNICODE | re.LOCALE)).lower()\n",
      "\n",
      "#donations = pd.read_csv('donations.csv')\n",
      "projects = pd.read_csv('projects.csv')\n",
      "outcomes = pd.read_csv('outcomes.csv')\n",
      "#resources = pd.read_csv('resources.csv')\n",
      "sample = pd.read_csv('sampleSubmission.csv')\n",
      "essays = pd.read_csv('essays.csv')\n",
      "\n",
      "\n",
      "essays = essays.sort('projectid')\n",
      "projects = projects.sort('projectid')\n",
      "sample = sample.sort('projectid')\n",
      "ess_proj = pd.merge(essays, projects, on='projectid')\n",
      "outcomes = outcomes.sort('projectid')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create Labels\n",
      "le = pre.LabelEncoder()\n",
      "le.fit(outcomes['is_exciting'])\n",
      "outcomes['is_exciting']=le.transform(outcomes['is_exciting'])\n",
      "labels=outcomes['is_exciting']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Clean essay data \n",
      "ess_proj['essay'] = ess_proj['essay'].apply(clean)\n",
      "ess_proj_arr = np.array(ess_proj)\n",
      "\n",
      "#get train indx and test indx\n",
      "#train_idx = np.where(ess_proj_arr[:,-1] < '2014-01-01')[0]\n",
      "train_idx=np.where(((ess_proj_arr[:,-1] > '2013-01-01') & (ess_proj_arr[:,-1] < '2014-01-01'))==True)[0]\n",
      "test_idx = np.where(ess_proj_arr[:,-1] >= '2014-01-01')[0]\n",
      "\n",
      "#ESSAY training Data\n",
      "traindata = ess_proj_arr[train_idx,:]\n",
      "testdata = ess_proj_arr[test_idx,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "(131089,)"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Manipute the data for the classifier\n",
      "tfidf = TfidfVectorizer(min_df=3,  max_features=1000)\n",
      "tfidf.fit(traindata[:,5])\n",
      "tr = tfidf.transform(traindata[:,5])\n",
      "ts = tfidf.transform(testdata[:,5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Utility function to report best scores\n",
      "def report(grid_scores, n_top=10):\n",
      "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
      "    for i, score in enumerate(top_scores):\n",
      "        print(\"Model with rank: {0}\".format(i + 1))\n",
      "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
      "              score.mean_validation_score,\n",
      "              np.std(score.cv_validation_scores)))\n",
      "        print(\"Parameters: {0}\".format(score.parameters))\n",
      "        print(\"\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load the already generated  training data for projecst.csv\n",
      "tr_proj=pickle.load(open('training_project_data_2013','rb'))\n",
      "ts_proj=pickle.load(open('testing_project_data','rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Combine projects.csv and essays.csv data for training and testing\n",
      "training_data=sps.hstack((tr,tr_proj),format='csr')\n",
      "testing_data=sps.hstack((ts,ts_proj),format='csr')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(131089, 41768)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Find the best parameters\n",
      "lr = linear_model.LogisticRegression()\n",
      "c_val= np.arange(0.001, 1, 0.1)\n",
      "param_grid = {\"penalty\": [\"l1\"],\n",
      "              \"class_weight\":[\"auto\"],\n",
      "              \"C\": c_val.tolist()}\n",
      "grid_search = GridSearchCV(lr, param_grid=param_grid,scoring='roc_auc')\n",
      "start = time()\n",
      "grid_search.fit(pre.scale(training_data,with_mean=False),labels[train_idx])\n",
      "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
      "      % (time() - start, len(grid_search.grid_scores_)))\n",
      "\n",
      "\n",
      "''' BEST Parameters by rank \n",
      "GridSearchCV took 11454.60 seconds for 10 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.700 (std: 0.001)\n",
      "Parameters: {'penalty': 'l1', 'C': 0.7010000000000001, 'class_weight': 'auto'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.700 (std: 0.001)\n",
      "Parameters: {'penalty': 'l1', 'C': 0.6010000000000001, 'class_weight': 'auto'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.700 (std: 0.001)\n",
      "Parameters: {'penalty': 'l1', 'C': 0.801, 'class_weight': 'auto'\n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Array contains NaN or infinity.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-90-20a849ddf63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n\u001b[1;32m     11\u001b[0m       % (time() - start, len(grid_search.grid_scores_)))\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    705\u001b[0m                           \u001b[0;34m\" The params argument will be removed in 0.15.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                           DeprecationWarning)\n\u001b[0;32m--> 707\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_lists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         self.scorer_ = _deprecate_loss_and_score_funcs(\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_arrays\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     25\u001b[0m     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n\u001b[1;32m     26\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array contains NaN or infinity.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Array contains NaN or infinity."
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = linear_model.LogisticRegression(penalty='l1',C=0.801,class_weight='auto')\n",
      "lr.fit(pre.scale(training_data,with_mean=False), labels[train_idx])\n",
      "preds =lr.predict_proba(pre.scale(testing_data,with_mean=False))[:,1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.isnan(np.min(training_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = svm.LinearSVC(loss='l1',class_weight='auto',C=0.5)\n",
      "clf.fit(pre.scale(training_data,with_mean=False), labels[train_idx]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preds=clf.predict(testing_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Write to File"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Write results to file\n",
      "\n",
      "sample['is_exciting'] = preds\n",
      "sample.to_csv('predictions.csv', index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}